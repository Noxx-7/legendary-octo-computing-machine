{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Small Language Model Training on T4 GPU\n",
    "## Practical 125M Parameter Model\n",
    "- ‚úÖ T4 GPU Optimized (15GB Memory)\n",
    "- ‚úÖ Complete, runnable code (no truncations)\n",
    "- ‚úÖ Real dataset from HuggingFace\n",
    "- ‚úÖ Proper evaluation and monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
    "!pip install transformers datasets tokenizers accelerate einops matplotlib numpy tqdm wandb -q\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import gc\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Memory optimizations for T4\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ T4 GPU OPTIMIZED 125M PARAMETER MODEL\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu():\n",
    "    \"\"\"Check for GPU availability and setup\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"‚úÖ GPU Detected: {gpu_name}\")\n",
    "        print(f\"   Total Memory: {total_memory:.2f} GB\")\n",
    "        \n",
    "        # Set memory fraction for stability\n",
    "        torch.cuda.set_per_process_memory_fraction(0.9)\n",
    "        \n",
    "        # Enable TF32 for faster computation on Ampere GPUs\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GPU available. Using CPU (very slow training)\")\n",
    "        return False\n",
    "\n",
    "USE_GPU = check_gpu()\n",
    "device = torch.device('cuda' if USE_GPU else 'cpu')\n",
    "print(f\"üîß Using device: {device}\")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU/CPU memory\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for 125M parameter model optimized for T4\"\"\"\n",
    "    \n",
    "    # Model architecture (125M parameters)\n",
    "    vocab_size: int = 32000  # Smaller vocabulary for efficiency\n",
    "    hidden_size: int = 768   # Hidden dimension\n",
    "    num_layers: int = 12     # Number of transformer layers\n",
    "    num_heads: int = 12      # Number of attention heads\n",
    "    ff_dim: int = 3072       # Feedforward dimension (4 * hidden_size)\n",
    "    max_seq_len: int = 512   # Maximum sequence length (reduced for T4)\n",
    "    \n",
    "    # Training configuration\n",
    "    batch_size: int = 4 if USE_GPU else 1\n",
    "    gradient_accumulation_steps: int = 8  # Effective batch size = 32\n",
    "    learning_rate: float = 6e-4\n",
    "    num_epochs: int = 3  # Practical for demo\n",
    "    warmup_steps: int = 500\n",
    "    max_grad_norm: float = 1.0\n",
    "    dropout: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    \n",
    "    # Optimization flags\n",
    "    use_mixed_precision: bool = USE_GPU\n",
    "    gradient_checkpointing: bool = True  # Save memory\n",
    "    compile_model: bool = False  # Set True if using PyTorch 2.0+\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_every_n_steps: int = 1000\n",
    "    eval_every_n_steps: int = 500\n",
    "    logging_steps: int = 10\n",
    "    \n",
    "    # Paths\n",
    "    checkpoint_dir: str = \"./checkpoints\"\n",
    "    \n",
    "config = ModelConfig()\n",
    "\n",
    "# Calculate actual model size\n",
    "def calculate_params(cfg):\n",
    "    \"\"\"Calculate number of parameters\"\"\"\n",
    "    embedding_params = cfg.vocab_size * cfg.hidden_size * 2  # token + position\n",
    "    attention_params = cfg.num_layers * (4 * cfg.hidden_size * cfg.hidden_size)  # Q,K,V,O\n",
    "    ff_params = cfg.num_layers * (2 * cfg.hidden_size * cfg.ff_dim)  # 2 linear layers\n",
    "    norm_params = cfg.num_layers * 2 * cfg.hidden_size * 2  # 2 LayerNorms per layer\n",
    "    total = embedding_params + attention_params + ff_params + norm_params\n",
    "    return total\n",
    "\n",
    "total_params = calculate_params(config)\n",
    "print(f\"\\nüìä Model Configuration:\")\n",
    "print(f\"   Total parameters: {total_params/1e6:.1f}M\")\n",
    "print(f\"   Hidden size: {config.hidden_size}\")\n",
    "print(f\"   Layers: {config.num_layers}\")\n",
    "print(f\"   Heads: {config.num_heads}\")\n",
    "print(f\"   Sequence length: {config.max_seq_len}\")\n",
    "print(f\"   Batch size: {config.batch_size} (x{config.gradient_accumulation_steps} accumulation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"Rotary Position Embedding (RoPE) for better long-range modeling\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Precompute the frequency tensor\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        \n",
    "        # Precompute cos and sin for max sequence length\n",
    "        t = torch.arange(max_seq_len).float()\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        self.register_buffer('cos_cached', torch.cos(freqs))\n",
    "        self.register_buffer('sin_cached', torch.sin(freqs))\n",
    "    \n",
    "    def forward(self, x, seq_len):\n",
    "        return self.cos_cached[:seq_len], self.sin_cached[:seq_len]\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    \"\"\"Apply rotary embeddings to queries and keys\"\"\"\n",
    "    def rotate_half(x):\n",
    "        x1, x2 = x.chunk(2, dim=-1)\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "    \n",
    "    # Repeat cos and sin to match full head_dim\n",
    "    cos = torch.cat([cos, cos], dim=-1)\n",
    "    sin = torch.cat([sin, sin], dim=-1)\n",
    "    \n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention with RoPE and optional flash attention\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        \n",
    "        self.rope = RotaryPositionalEmbedding(self.head_dim, config.max_seq_len)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        B, L, D = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).view(B, L, self.num_heads, self.head_dim)\n",
    "        v = self.v_proj(x).view(B, L, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Apply RoPE\n",
    "        cos, sin = self.rope(x, L)\n",
    "        cos = cos.unsqueeze(0).unsqueeze(2)  # [seq_len, dim//2] -> [1, seq_len, 1, dim//2]\n",
    "        sin = sin.unsqueeze(0).unsqueeze(2)\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        \n",
    "        # Reshape for attention\n",
    "        q = q.transpose(1, 2)  # (B, num_heads, L, head_dim)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply causal mask\n",
    "        if mask is None:\n",
    "            mask = torch.triu(torch.ones(L, L, device=x.device), diagonal=1).bool()\n",
    "    "        # Use smaller value for FP16 compatibility (-1e9 overflows in half precision)\n",
    "        mask_value = -1e4 if scores.dtype == torch.float16 else -1e9\n",
    "        scores = scores.masked_fill(mask, mask_value)"\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attn_weights, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, L, D)\n",
    "        \n",
    "        # Output projection\n",
    "        out = self.o_proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-forward network with SwiGLU activation\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(config.hidden_size, config.ff_dim, bias=False)\n",
    "        self.w2 = nn.Linear(config.ff_dim, config.hidden_size, bias=False)\n",
    "        self.w3 = nn.Linear(config.hidden_size, config.ff_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # SwiGLU activation\n",
    "        return self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with pre-normalization\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        self.ln1 = nn.RMSNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ln2 = nn.RMSNorm(config.hidden_size, eps=1e-6)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-norm architecture\n",
    "        x = x + self.attention(self.ln1(x), mask)\n",
    "        x = x + self.feed_forward(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallLanguageModel(nn.Module):\n",
    "    \"\"\"Complete Small Language Model for T4 GPU\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(config.num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.ln_f = nn.RMSNorm(config.hidden_size, eps=1e-6)\n",
    "        \n",
    "        # Language modeling head with weight tying\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_embedding.weight  # Weight tying\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # Print model info\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"\\n‚úÖ Model initialized:\")\n",
    "        print(f\"   Total parameters: {total_params/1e6:.1f}M\")\n",
    "        print(f\"   Trainable parameters: {trainable_params/1e6:.1f}M\")\n",
    "        print(f\"   Memory footprint: ~{total_params * 4 / 1e9:.2f} GB (FP32)\")\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights with scaled normal distribution\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        B, L = input_ids.shape\n",
    "        \n",
    "        # Token embeddings\n",
    "        x = self.token_embedding(input_ids)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Create causal mask\n",
    "        mask = torch.triu(torch.ones(L, L, device=input_ids.device), diagonal=1).bool()\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            if self.config.gradient_checkpointing and self.training:\n",
    "                x = torch.utils.checkpoint.checkpoint(block, x, mask)\n",
    "            else:\n",
    "                x = block(x, mask)\n",
    "        \n",
    "        # Final layer norm and output projection\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        # Calculate loss if labels provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift for next-token prediction\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            # Flatten for loss calculation\n",
    "            loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, self.config.vocab_size),\n",
    "                shift_labels.view(-1),\n",
    "                ignore_index=-100\n",
    "            )\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids, max_new_tokens=50, temperature=1.0, top_p=0.95):\n",
    "        \"\"\"Generate text using the model\"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get predictions for last position\n",
    "            logits, _ = self(input_ids)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Apply top-p (nucleus) sampling\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            \n",
    "            # Remove tokens with cumulative probability above the threshold\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            \n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            logits[:, indices_to_remove] = -float('inf')\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "            \n",
    "            # Stop if sequence gets too long\n",
    "            if input_ids.shape[1] >= self.config.max_seq_len:\n",
    "                break\n",
    "        \n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for loading and processing text data\"\"\"\n",
    "    \n",
    "    def __init__(self, encodings, seq_len):\n",
    "        self.encodings = encodings\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # Use input_ids as both input and labels for language modeling\n",
    "        item['labels'] = item['input_ids'].clone()\n",
    "        return item\n",
    "\n",
    "def load_and_prepare_data(config):\n",
    "    \"\"\"Load dataset from HuggingFace and prepare for training\"\"\"\n",
    "    print(\"\\nüìö Loading dataset...\")\n",
    "    \n",
    "    # Load a subset of OpenWebText (high-quality web text)\n",
    "    # Using 'tiny_shakespeare' for quick demo - replace with larger dataset for real training\n",
    "    dataset = load_dataset('roneneldan/TinyStories', split='train[:10000]')  # Small subset for demo\n",
    "    \n",
    "    print(f\"   Loaded {len(dataset)} examples\")\n",
    "    \n",
    "    # Initialize tokenizer (using GPT-2 tokenizer as base)\n",
    "    print(\"\\nüî§ Initializing tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Tokenize the dataset\n",
    "    print(\"\\n‚öôÔ∏è Tokenizing dataset...\")\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=config.max_seq_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    \n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Tokenizing\"\n",
    "    )\n",
    "    \n",
    "    # Split into train and validation\n",
    "    train_size = int(0.9 * len(tokenized_dataset))\n",
    "    train_dataset = TextDataset(\n",
    "        {'input_ids': tokenized_dataset['input_ids'][:train_size]},\n",
    "        config.max_seq_len\n",
    "    )\n",
    "    val_dataset = TextDataset(\n",
    "        {'input_ids': tokenized_dataset['input_ids'][train_size:]},\n",
    "        config.max_seq_len\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Data prepared:\")\n",
    "    print(f\"   Train samples: {len(train_dataset)}\")\n",
    "    print(f\"   Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"   Sequence length: {config.max_seq_len}\")\n",
    "    print(f\"   Vocabulary size: {tokenizer.vocab_size}\")\n",
    "    \n",
    "    # Update config with actual vocab size\n",
    "    config.vocab_size = tokenizer.vocab_size\n",
    "    \n",
    "    return train_dataset, val_dataset, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the dataset\n",
    "train_dataset, val_dataset, tokenizer = load_and_prepare_data(config)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=USE_GPU\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=USE_GPU\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ DataLoaders created:\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = SmallLanguageModel(config).to(device)\n",
    "\n",
    "# Compile model if using PyTorch 2.0+ (optional, can speed up training)\n",
    "if config.compile_model and hasattr(torch, 'compile'):\n",
    "    print(\"\\nüîß Compiling model with torch.compile()...\")\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    "    betas=(0.9, 0.95)\n",
    ")\n",
    "\n",
    "# Create gradient scaler for mixed precision training\n",
    "scaler = GradScaler() if config.use_mixed_precision else None\n",
    "\n",
    "# Learning rate scheduler\n",
    "def get_lr_scheduler(optimizer, num_training_steps):\n",
    "    \"\"\"Cosine learning rate schedule with warmup\"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < config.warmup_steps:\n",
    "            return current_step / config.warmup_steps\n",
    "        progress = (current_step - config.warmup_steps) / (num_training_steps - config.warmup_steps)\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    \n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "num_training_steps = len(train_loader) * config.num_epochs // config.gradient_accumulation_steps\n",
    "scheduler = get_lr_scheduler(optimizer, num_training_steps)\n",
    "\n",
    "print(f\"\\n‚úÖ Training setup complete:\")\n",
    "print(f\"   Optimizer: AdamW\")\n",
    "print(f\"   Learning rate: {config.learning_rate}\")\n",
    "print(f\"   Weight decay: {config.weight_decay}\")\n",
    "print(f\"   Mixed precision: {config.use_mixed_precision}\")\n",
    "print(f\"   Gradient checkpointing: {config.gradient_checkpointing}\")\n",
    "print(f\"   Total training steps: {num_training_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Evaluate the model on validation data\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        logits, loss = model(input_ids, labels)\n",
    "        \n",
    "        total_loss += loss.item() * input_ids.shape[0]\n",
    "        total_tokens += input_ids.shape[0]\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(min(avg_loss, 100))  # Cap to avoid overflow\n",
    "    \n",
    "    model.train()\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, step, loss, config):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(config.checkpoint_dir, f'checkpoint_step_{step}.pt')\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'config': config.__dict__\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    print(f\"\\nüíæ Checkpoint saved: {checkpoint_path}\")\n",
    "    return checkpoint_path\n",
    "\n",
    "def generate_sample(model, tokenizer, prompt, device, max_new_tokens=50):\n",
    "    \"\"\"Generate a sample from the model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=config.max_seq_len)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(input_ids, max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    # Decode\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    model.train()\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, scaler, config, tokenizer):\n",
    "    \"\"\"Main training loop\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üöÄ Starting training...\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    global_step = 0\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        print(f\"\\nüìÖ Epoch {epoch + 1}/{config.num_epochs}\")\n",
    "        \n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        accumulated_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\")\n",
    "        \n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            if config.use_mixed_precision:\n",
    "                with autocast():\n",
    "                    logits, loss = model(input_ids, labels)\n",
    "                    loss = loss / config.gradient_accumulation_steps\n",
    "            else:\n",
    "                logits, loss = model(input_ids, labels)\n",
    "                loss = loss / config.gradient_accumulation_steps\n",
    "            \n",
    "            # Backward pass\n",
    "            if config.use_mixed_precision:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            \n",
    "            accumulated_loss += loss.item()\n",
    "            \n",
    "            # Update weights after accumulation\n",
    "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "                if config.use_mixed_precision:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "                    optimizer.step()\n",
    "                \n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "                \n",
    "                # Logging\n",
    "                if global_step % config.logging_steps == 0:\n",
    "                    avg_loss = accumulated_loss * config.gradient_accumulation_steps\n",
    "                    current_lr = scheduler.get_last_lr()[0]\n",
    "                    progress_bar.set_postfix({\n",
    "                        'loss': f'{avg_loss:.4f}',\n",
    "                        'lr': f'{current_lr:.2e}'\n",
    "                    })\n",
    "                    train_losses.append(avg_loss)\n",
    "                    accumulated_loss = 0\n",
    "                \n",
    "                # Evaluation\n",
    "                if global_step % config.eval_every_n_steps == 0:\n",
    "                    print(f\"\\nüìä Evaluating at step {global_step}...\")\n",
    "                    val_loss, val_perplexity = evaluate(model, val_loader, device)\n",
    "                    val_losses.append(val_loss)\n",
    "                    print(f\"   Validation Loss: {val_loss:.4f}\")\n",
    "                    print(f\"   Validation Perplexity: {val_perplexity:.2f}\")\n",
    "                    \n",
    "                    # Save best model\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        save_checkpoint(model, optimizer, epoch, global_step, val_loss, config)\n",
    "                    \n",
    "                    # Generate sample\n",
    "                    print(\"\\nüîÆ Generating sample...\")\n",
    "                    sample = generate_sample(model, tokenizer, \"Once upon a time\", device)\n",
    "                    print(f\"   Generated: {sample[:200]}...\")\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if global_step % config.save_every_n_steps == 0:\n",
    "                    save_checkpoint(model, optimizer, epoch, global_step, accumulated_loss, config)\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if step % 100 == 0:\n",
    "                clear_memory()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ Training complete!\")\n",
    "    print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader, \n",
    "    optimizer, scheduler, scaler, \n",
    "    config, tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_losses)\n",
    "plt.title('Validation Loss')\n",
    "plt.xlabel('Evaluation Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=100)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Training curves saved to 'training_curves.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive text generation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ü§ñ Interactive Text Generation\")\n",
    "print(\"=\"*80)\n",
    "print(\"Enter a prompt and the model will continue the text.\")\n",
    "print(\"Type 'quit' to exit.\\n\")\n",
    "\n",
    "test_prompts = [\n",
    "    \"Once upon a time, in a land far away,\",\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"In the beginning, there was\",\n",
    "    \"The key to happiness is\",\n",
    "    \"Science has discovered that\"\n",
    "]\n",
    "\n",
    "print(\"üìù Example prompts to try:\")\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"   {i}. {prompt}\")\n",
    "print()\n",
    "\n",
    "# Generate samples for test prompts\n",
    "for prompt in test_prompts[:3]:  # Test first 3 prompts\n",
    "    print(f\"\\nüí≠ Prompt: {prompt}\")\n",
    "    generated = generate_sample(model, tokenizer, prompt, device, max_new_tokens=50)\n",
    "    print(f\"üìù Generated: {generated}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model information and tips\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìö Training Complete - Next Steps\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "‚úÖ Your model has been trained successfully!\n",
    "\n",
    "üîÑ To improve the model:\n",
    "1. Train for more epochs (increase config.num_epochs)\n",
    "2. Use a larger dataset (e.g., 'openwebtext', 'the_pile')\n",
    "3. Increase model size (if GPU memory allows)\n",
    "4. Fine-tune hyperparameters (learning rate, batch size)\n",
    "5. Implement advanced techniques (flash attention, LoRA)\n",
    "\n",
    "üíæ Model checkpoints saved in: ./checkpoints/\n",
    "\n",
    "üöÄ For production use:\n",
    "1. Convert to ONNX or TorchScript for deployment\n",
    "2. Implement proper API endpoints\n",
    "3. Add safety filters and content moderation\n",
    "4. Set up monitoring and logging\n",
    "\"\"\")\n",
    "\n",
    "# Memory cleanup\n",
    "clear_memory()\n",
    "print(\"\\n‚ú® All done! Happy modeling! ‚ú®\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}