{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Small Language Model Training - Enhanced Edition\n",
    "## 125M Parameter Model with Large Dataset Support\n",
    "- ‚úÖ T4 GPU Optimized (15GB Memory)\n",
    "- ‚úÖ Support for large-scale datasets (OpenWebText, Wikipedia, C4)\n",
    "- ‚úÖ All bugs fixed (RoPE dimensions, FP16 overflow)\n",
    "- ‚úÖ Production-ready implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
    "!pip install transformers datasets tokenizers accelerate einops matplotlib numpy tqdm -q\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import gc\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Memory optimizations for T4\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ ENHANCED SLM TRAINER - LARGE DATASET SUPPORT\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu():\n",
    "    \"\"\"Check for GPU availability and setup\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"‚úÖ GPU Detected: {gpu_name}\")\n",
    "        print(f\"   Total Memory: {total_memory:.2f} GB\")\n",
    "        \n",
    "        # Set memory fraction for stability\n",
    "        torch.cuda.set_per_process_memory_fraction(0.9)\n",
    "        \n",
    "        # Enable TF32 for faster computation on Ampere GPUs\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No GPU available. Using CPU (very slow)\")\n",
    "        return False\n",
    "\n",
    "USE_GPU = check_gpu()\n",
    "device = torch.device('cuda' if USE_GPU else 'cpu')\n",
    "print(f\"üîß Using device: {device}\")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU/CPU memory\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Dataset Selection\n",
    "Choose from multiple large-scale datasets for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT YOUR DATASET HERE\n",
    "# Options: 'tinystories', 'openwebtext', 'wikipedia', 'bookcorpus', 'c4', 'pile'\n",
    "DATASET_NAME = 'tinystories'  # Change this to use larger datasets\n",
    "MAX_SAMPLES = 100000  # Set to None for full dataset\n",
    "\n",
    "print(f\"üìä Dataset Information:\")\n",
    "print(f\"   {'TinyStories':<15} - 2M stories, ~500MB (good for testing)\")\n",
    "print(f\"   {'OpenWebText':<15} - 8M documents, ~40GB (GPT-2 quality)\")\n",
    "print(f\"   {'Wikipedia':<15} - 6M articles, ~20GB (clean, factual)\")\n",
    "print(f\"   {'BookCorpus':<15} - 74M sentences, ~5GB (books)\")\n",
    "print(f\"   {'C4':<15} - 365M documents, ~300GB (massive web crawl)\")\n",
    "print(f\"   {'The Pile':<15} - 800GB diverse text (best quality)\")\n",
    "print(f\"\\n‚úÖ Selected: {DATASET_NAME}\")\n",
    "if MAX_SAMPLES:\n",
    "    print(f\"   Limited to {MAX_SAMPLES:,} samples for demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for 125M parameter model optimized for T4\"\"\"\n",
    "    \n",
    "    # Model architecture (125M parameters)\n",
    "    vocab_size: int = 50257  # GPT-2 vocabulary\n",
    "    hidden_size: int = 768   # Hidden dimension\n",
    "    num_layers: int = 12     # Number of transformer layers\n",
    "    num_heads: int = 12      # Number of attention heads\n",
    "    ff_dim: int = 3072       # Feedforward dimension\n",
    "    max_seq_len: int = 512   # Maximum sequence length\n",
    "    \n",
    "    # Training configuration\n",
    "    batch_size: int = 8 if USE_GPU else 2\n",
    "    gradient_accumulation_steps: int = 4  # Effective batch = 32\n",
    "    learning_rate: float = 6e-4\n",
    "    num_epochs: int = 1  # Start with 1 for large datasets\n",
    "    warmup_steps: int = 1000\n",
    "    max_grad_norm: float = 1.0\n",
    "    dropout: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    \n",
    "    # Memory optimizations\n",
    "    gradient_checkpointing: bool = True\n",
    "    use_mixed_precision: bool = USE_GPU\n",
    "    \n",
    "    # Logging\n",
    "    log_interval: int = 50\n",
    "    eval_interval: int = 500\n",
    "    save_interval: int = 1000\n",
    "    \n",
    "    def model_size(self):\n",
    "        \"\"\"Calculate approximate model size in millions of parameters\"\"\"\n",
    "        # Embedding parameters\n",
    "        embedding_params = self.vocab_size * self.hidden_size * 2\n",
    "        \n",
    "        # Transformer layer parameters\n",
    "        attention_params = 4 * self.hidden_size * self.hidden_size  # Q,K,V,O projections\n",
    "        ff_params = 3 * self.hidden_size * self.ff_dim  # W1, W2, W3 for SwiGLU\n",
    "        norm_params = 2 * self.hidden_size  # Two RMSNorms per layer\n",
    "        layer_params = attention_params + ff_params + norm_params\n",
    "        \n",
    "        # Total\n",
    "        total_params = embedding_params + (layer_params * self.num_layers) + self.hidden_size\n",
    "        return total_params / 1e6\n",
    "\n",
    "config = ModelConfig()\n",
    "print(f\"\\nüìä Model Configuration:\")\n",
    "print(f\"   Parameters: ~{config.model_size():.1f}M\")\n",
    "print(f\"   Memory footprint: ~{config.model_size() * 4 / 1000:.2f} GB (FP32)\")\n",
    "print(f\"   Effective batch size: {config.batch_size * config.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Large Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_large_dataset(dataset_name, max_samples=None):\n",
    "    \"\"\"Load various large-scale datasets for training\"\"\"\n",
    "    print(f\"üìö Loading {dataset_name} dataset...\")\n",
    "    \n",
    "    if dataset_name == 'tinystories':\n",
    "        # Small dataset for testing\n",
    "        split = f\"train[:{max_samples}]\" if max_samples else \"train\"\n",
    "        dataset = load_dataset(\"roneneldan/TinyStories\", split=split)\n",
    "        val_split = f\"validation[:1000]\"\n",
    "        val_dataset = load_dataset(\"roneneldan/TinyStories\", split=val_split)\n",
    "        \n",
    "    elif dataset_name == 'openwebtext':\n",
    "        # High-quality web text (GPT-2 training data)\n",
    "        split = f\"train[:{max_samples}]\" if max_samples else \"train\"\n",
    "        dataset = load_dataset(\"Skylion007/openwebtext\", split=split)\n",
    "        # Create validation split from last 5000 samples\n",
    "        val_dataset = load_dataset(\"Skylion007/openwebtext\", split=\"train[-5000:]\")\n",
    "        \n",
    "    elif dataset_name == 'wikipedia':\n",
    "        # Wikipedia English\n",
    "        split = f\"train[:{max_samples}]\" if max_samples else \"train\"\n",
    "        dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=split)\n",
    "        val_dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[-5000:]\")\n",
    "        \n",
    "    elif dataset_name == 'bookcorpus':\n",
    "        # Books dataset\n",
    "        split = f\"train[:{max_samples}]\" if max_samples else \"train\"\n",
    "        dataset = load_dataset(\"bookcorpusopen\", split=split)\n",
    "        val_dataset = load_dataset(\"bookcorpusopen\", split=\"train[-5000:]\")\n",
    "        \n",
    "    elif dataset_name == 'c4':\n",
    "        # Colossal Clean Crawled Corpus\n",
    "        samples = max_samples if max_samples else 1000000\n",
    "        split = f\"train[:{samples}]\"\n",
    "        dataset = load_dataset(\"c4\", \"en\", split=split, streaming=False)\n",
    "        val_dataset = load_dataset(\"c4\", \"en\", split=\"validation[:5000]\")\n",
    "        \n",
    "    elif dataset_name == 'pile':\n",
    "        # The Pile - highest quality diverse dataset\n",
    "        samples = max_samples if max_samples else 100000\n",
    "        split = f\"train[:{samples}]\"\n",
    "        dataset = load_dataset(\"EleutherAI/pile\", split=split, streaming=False)\n",
    "        val_dataset = load_dataset(\"EleutherAI/pile\", split=\"validation[:5000]\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(dataset):,} training samples\")\n",
    "    print(f\"‚úÖ Loaded {len(val_dataset):,} validation samples\")\n",
    "    \n",
    "    return dataset, val_dataset\n",
    "\n",
    "# Load the selected dataset\n",
    "train_dataset, val_dataset = load_large_dataset(DATASET_NAME, MAX_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "print(\"üî§ Initializing tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "config.vocab_size = len(tokenizer)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Handle different field names\n",
    "    text_field = 'text' if 'text' in examples else 'story' \n",
    "    return tokenizer(\n",
    "        examples[text_field],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=config.max_seq_len,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "print(\"‚öôÔ∏è Tokenizing datasets...\")\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=val_dataset.column_names)\n",
    "\n",
    "print(f\"‚úÖ Tokenization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenized_dataset):\n",
    "        self.dataset = tokenized_dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        input_ids = torch.tensor(item['input_ids'], dtype=torch.long)\n",
    "        labels = input_ids.clone()\n",
    "        return input_ids, labels\n",
    "\n",
    "train_dataset = TextDataset(tokenized_train)\n",
    "val_dataset = TextDataset(tokenized_val)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=2 if USE_GPU else 0,\n",
    "    pin_memory=USE_GPU\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config.batch_size * 2, \n",
    "    shuffle=False,\n",
    "    num_workers=2 if USE_GPU else 0,\n",
    "    pin_memory=USE_GPU\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Data Statistics:\")\n",
    "print(f\"   Training batches: {len(train_loader):,}\")\n",
    "print(f\"   Validation batches: {len(val_loader):,}\")\n",
    "print(f\"   Tokens per epoch: ~{len(train_loader) * config.batch_size * config.max_seq_len:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Model Architecture (with fixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        norm = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "        return norm * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"Rotary Position Embedding (RoPE)\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Precompute the frequency tensor\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        \n",
    "        # Precompute cos and sin for max sequence length\n",
    "        t = torch.arange(max_seq_len).float()\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        self.register_buffer('cos_cached', torch.cos(freqs))\n",
    "        self.register_buffer('sin_cached', torch.sin(freqs))\n",
    "    \n",
    "    def forward(self, x, seq_len):\n",
    "        return self.cos_cached[:seq_len], self.sin_cached[:seq_len]\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    \"\"\"Apply rotary embeddings to queries and keys\"\"\"\n",
    "    def rotate_half(x):\n",
    "        x1, x2 = x.chunk(2, dim=-1)\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "    \n",
    "    # Repeat cos and sin to match full head_dim (FIX for dimension mismatch)\n",
    "    cos = torch.cat([cos, cos], dim=-1)\n",
    "    sin = torch.cat([sin, sin], dim=-1)\n",
    "    \n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention with RoPE\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        \n",
    "        self.rope = RotaryPositionalEmbedding(self.head_dim, config.max_seq_len)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        B, L, D = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).view(B, L, self.num_heads, self.head_dim)\n",
    "        v = self.v_proj(x).view(B, L, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Apply RoPE\n",
    "        cos, sin = self.rope(x, L)\n",
    "        cos = cos.unsqueeze(0).unsqueeze(2)  # [seq_len, dim//2] -> [1, seq_len, 1, dim//2]\n",
    "        sin = sin.unsqueeze(0).unsqueeze(2)\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        \n",
    "        # Reshape for attention\n",
    "        q = q.transpose(1, 2)  # (B, num_heads, L, head_dim)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply causal mask\n",
    "        if mask is None:\n",
    "            mask = torch.triu(torch.ones(L, L, device=x.device), diagonal=1).bool()\n",
    "        # Use smaller value for FP16 compatibility (FIX for overflow)\n",
    "        mask_value = -1e4 if scores.dtype == torch.float16 else -1e9\n",
    "        scores = scores.masked_fill(mask, mask_value)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attn_weights, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, L, D)\n",
    "        \n",
    "        # Output projection\n",
    "        out = self.o_proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-forward network with SwiGLU activation\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(config.hidden_size, config.ff_dim, bias=False)\n",
    "        self.w2 = nn.Linear(config.ff_dim, config.hidden_size, bias=False)\n",
    "        self.w3 = nn.Linear(config.hidden_size, config.ff_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # SwiGLU activation: swish(W1(x)) * W3(x)\n",
    "        return self.w2(self.dropout(F.silu(self.w1(x)) * self.w3(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with pre-normalization\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "        self.ln1 = RMSNorm(config.hidden_size)\n",
    "        self.ln2 = RMSNorm(config.hidden_size)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-norm architecture\n",
    "        x = x + self.attention(self.ln1(x), mask)\n",
    "        x = x + self.feed_forward(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallLanguageModel(nn.Module):\n",
    "    \"\"\"125M parameter language model optimized for T4\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(config.num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output layers\n",
    "        self.ln_f = RMSNorm(config.hidden_size)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying\n",
    "        self.lm_head.weight = self.token_embedding.weight\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        # Token embeddings\n",
    "        x = self.token_embedding(input_ids)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Create causal mask\n",
    "        B, L = input_ids.shape\n",
    "        mask = torch.triu(torch.ones(L, L, device=input_ids.device), diagonal=1).bool()\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            if self.config.gradient_checkpointing and self.training:\n",
    "                x = torch.utils.checkpoint.checkpoint(block, x, mask)\n",
    "            else:\n",
    "                x = block(x, mask)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        # Calculate loss if labels provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids, max_length=100, temperature=0.8, top_p=0.9):\n",
    "        \"\"\"Generate text using the model\"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        for _ in range(max_length - input_ids.shape[1]):\n",
    "            # Forward pass\n",
    "            logits, _ = self(input_ids)\n",
    "            \n",
    "            # Get next token logits\n",
    "            next_token_logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Apply top-p (nucleus) sampling\n",
    "            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            \n",
    "            # Remove tokens with cumulative probability above threshold\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            \n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "            next_token_logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            # Sample\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "            \n",
    "            # Stop if EOS token\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "        \n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = SmallLanguageModel(config).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"‚úÖ Model initialized:\")\n",
    "print(f\"   Total parameters: {total_params/1e6:.1f}M\")\n",
    "print(f\"   Trainable parameters: {trainable_params/1e6:.1f}M\")\n",
    "print(f\"   Memory footprint: ~{total_params * 4 / 1e9:.2f} GB (FP32)\")\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    "    betas=(0.9, 0.95)\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * config.num_epochs\n",
    "def get_lr_lambda(current_step):\n",
    "    # Warmup\n",
    "    if current_step < config.warmup_steps:\n",
    "        return current_step / config.warmup_steps\n",
    "    # Cosine decay\n",
    "    progress = (current_step - config.warmup_steps) / (total_steps - config.warmup_steps)\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=get_lr_lambda)\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler() if config.use_mixed_precision else None\n",
    "\n",
    "print(\"‚úÖ Training setup complete\")\n",
    "print(f\"   Optimizer: AdamW\")\n",
    "print(f\"   Learning rate: {config.learning_rate}\")\n",
    "print(f\"   Warmup steps: {config.warmup_steps}\")\n",
    "print(f\"   Mixed precision: {config.use_mixed_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, scaler, config, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=f\"Training Epoch {epoch}\")\n",
    "    \n",
    "    for i, (input_ids, labels) in enumerate(progress_bar):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        if config.use_mixed_precision:\n",
    "            with autocast():\n",
    "                logits, loss = model(input_ids, labels)\n",
    "                loss = loss / config.gradient_accumulation_steps\n",
    "        else:\n",
    "            logits, loss = model(input_ids, labels)\n",
    "            loss = loss / config.gradient_accumulation_steps\n",
    "        \n",
    "        # Backward pass\n",
    "        if config.use_mixed_precision:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        if (i + 1) % config.gradient_accumulation_steps == 0:\n",
    "            if config.use_mixed_precision:\n",
    "                scaler.unscale_(optimizer)\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "            \n",
    "            if config.use_mixed_precision:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            \n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * config.gradient_accumulation_steps\n",
    "        \n",
    "        # Update progress bar\n",
    "        if i % config.log_interval == 0:\n",
    "            avg_loss = total_loss / (i + 1)\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{avg_loss:.4f}',\n",
    "                'lr': f'{current_lr:.2e}',\n",
    "                'ppl': f'{math.exp(avg_loss):.2f}'\n",
    "            })\n",
    "        \n",
    "        # Free memory periodically\n",
    "        if i % 100 == 0:\n",
    "            clear_memory()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, config):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for input_ids, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        logits, loss = model(input_ids, labels)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_samples(model, tokenizer, prompts, max_length=100, temperature=0.8):\n",
    "    \"\"\"Generate text samples\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nüìù Prompt: {prompt}\")\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "        \n",
    "        # Generate\n",
    "        output_ids = model.generate(input_ids, max_length=max_length, temperature=temperature)\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        print(f\"üí¨ Generated: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Epochs: {config.num_epochs}\")\n",
    "print(f\"Batch size: {config.batch_size} x {config.gradient_accumulation_steps} = {config.batch_size * config.gradient_accumulation_steps}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test generation before training\n",
    "print(\"\\nüîÆ Testing generation (untrained model)...\")\n",
    "test_prompts = [\"Once upon a time\", \"The future of AI is\"]\n",
    "generate_samples(model, tokenizer, test_prompts, max_length=50)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, config.num_epochs + 1):\n",
    "    print(f\"\\nüìÖ Epoch {epoch}/{config.num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, scaler, config, epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_ppl = evaluate(model, val_loader, config)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"\\nüìä Epoch {epoch} Results:\")\n",
    "    print(f\"   Train Loss: {train_loss:.4f} | Perplexity: {math.exp(train_loss):.2f}\")\n",
    "    print(f\"   Val Loss: {val_loss:.4f} | Perplexity: {val_ppl:.2f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'config': config\n",
    "        }, f'best_model_{DATASET_NAME}.pt')\n",
    "        print(f\"   üíæ Saved best model (val_loss: {val_loss:.4f})\")\n",
    "    \n",
    "    # Generate samples\n",
    "    print(\"\\nüîÆ Generating samples...\")\n",
    "    generate_samples(model, tokenizer, test_prompts, max_length=50)\n",
    "    \n",
    "    clear_memory()\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([math.exp(l) for l in train_losses], label='Train Perplexity')\n",
    "plt.plot([math.exp(l) for l in val_losses], label='Val Perplexity')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.title('Perplexity Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'training_curves_{DATASET_NAME}.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìà Final Results:\")\n",
    "print(f\"   Best Val Loss: {best_val_loss:.4f}\")\n",
    "print(f\"   Best Val Perplexity: {math.exp(best_val_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Interactive Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_generation():\n",
    "    \"\"\"Interactive text generation interface\"\"\"\n",
    "    print(\"\\nüé® Interactive Text Generation\")\n",
    "    print(\"Type 'quit' to exit\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    while True:\n",
    "        prompt = input(\"\\nEnter prompt: \")\n",
    "        if prompt.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            max_len = int(input(\"Max length (default 100): \") or 100)\n",
    "            temp = float(input(\"Temperature 0.1-2.0 (default 0.8): \") or 0.8)\n",
    "        except ValueError:\n",
    "            max_len = 100\n",
    "            temp = 0.8\n",
    "        \n",
    "        print(\"\\nü§ñ Generating...\")\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "        output_ids = model.generate(input_ids, max_length=max_len, temperature=temp)\n",
    "        generated = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"\\nüìù Generated text:\\n{generated}\")\n",
    "\n",
    "# Run interactive generation\n",
    "interactive_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model\n",
    "print(\"\\nüìä Model Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Parameter count by component\n",
    "components = {\n",
    "    'Embeddings': sum(p.numel() for n, p in model.named_parameters() if 'embedding' in n),\n",
    "    'Attention': sum(p.numel() for n, p in model.named_parameters() if 'attention' in n or 'q_proj' in n or 'k_proj' in n or 'v_proj' in n or 'o_proj' in n),\n",
    "    'FeedForward': sum(p.numel() for n, p in model.named_parameters() if 'w1' in n or 'w2' in n or 'w3' in n),\n",
    "    'Normalization': sum(p.numel() for n, p in model.named_parameters() if 'ln' in n or 'norm' in n),\n",
    "    'Output': sum(p.numel() for n, p in model.named_parameters() if 'lm_head' in n)\n",
    "}\n",
    "\n",
    "total_params = sum(components.values())\n",
    "for name, count in components.items():\n",
    "    pct = (count / total_params) * 100\n",
    "    print(f\"{name:<15}: {count/1e6:>8.2f}M ({pct:>5.1f}%)\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Total':<15}: {total_params/1e6:>8.2f}M\")\n",
    "\n",
    "# Memory usage\n",
    "if USE_GPU:\n",
    "    print(f\"\\nüíæ GPU Memory:\")\n",
    "    print(f\"   Allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    print(f\"   Reserved: {torch.cuda.memory_reserved()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Tips for Large-Scale Training\n",
    "\n",
    "### Dataset Recommendations by Model Size:\n",
    "- **125M parameters**: 1-10B tokens minimum\n",
    "- **350M parameters**: 10-50B tokens \n",
    "- **1B parameters**: 50-200B tokens\n",
    "- **3B+ parameters**: 200B+ tokens\n",
    "\n",
    "### Training Time Estimates (T4 GPU):\n",
    "- **TinyStories** (2M samples): ~30 minutes/epoch\n",
    "- **OpenWebText** (8M samples): ~8 hours/epoch\n",
    "- **Wikipedia** (6M articles): ~6 hours/epoch\n",
    "- **C4** (365M samples): ~7 days/epoch\n",
    "\n",
    "### Memory Optimization Tips:\n",
    "1. Use gradient checkpointing (already enabled)\n",
    "2. Reduce batch size if OOM\n",
    "3. Use mixed precision training (FP16)\n",
    "4. Consider gradient accumulation\n",
    "5. Use DeepSpeed or FSDP for multi-GPU\n",
    "\n",
    "### Quality Improvements:\n",
    "1. Train for more epochs (3-10)\n",
    "2. Use larger, cleaner datasets\n",
    "3. Implement learning rate decay\n",
    "4. Add dropout and weight decay\n",
    "5. Use validation for early stopping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}